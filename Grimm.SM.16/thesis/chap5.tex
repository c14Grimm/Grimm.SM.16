%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Tensor-Based Value Iteration for Pursuit-Evasion Games}\label{chp:tvipe}
Building on the work presented in \Cref{chp:tvi}, this chapter will present an algorithm that uses TVI to solve pursuit-evasion games. This chapter will begin by describing the general dynamic programming problem used to model a pursuit-evasion game. Next, the chapter will detail how best response can be used along with dynamic programming to solve for the optimal control of both the pursuer and the evader. The Best Response Dynamic Programming algorithm will then be presented in detail. Finally, the chapter will culminate in the presentation of the Best Response Tensor-Based Value Iteration algorithm.  

\section{Description of Problem}\label{peprobdes}
As demonstrated in \Cref{gameqn} of \Cref{pegames}, solving pursuit-evasion games requires determining optimal control values for two or more players with opposing outcomes. In order to keep the problem as simple as possible, there will be a focus on zero-sum games in which the cost function of the pursuer is the negative of the evaders cost function or $(G_1(x) = -G_2(x))$. Despite this constraint, it is possible to apply TVI to nonzero-sum games or even simply use predetermined controls as either the pursuer or evaders control values. Before exploring the modifications brought about by adjusting TVI for pursuit-evasion games, a description of how dynamic programming may be used with best response will be provided.

Value iteration can be modified to solve pursuit-evasion games using a best response dynamic by solving for the optimal cost at each state. The problem is modeled as a discounted, deterministic, shortest path problem where $\gamma$ is the discount factor. First, the state space $Z$ is discretized into evenly spaced states $z_i \in Z$ where the space between states are $h$ just as in \cite{bardi2}. Given that $z_i$ is the current state, $u_1$ is the pursuer control, and $u_2$ is the evader control, $P(z_j|z_i,u_1,u_2) = 1$ where $z_j$ is the deterministic result of $f(z_i,u_1,u_2)$. Even if $z_j \notin z_i$, the state cost at $z_j$ can be determined by:
\begin{equation}\label{concomb}
J(z_j)  =  \sum_i\lambda_iJ(z_i),
\end{equation}
where:
\begin{equation}\label{lamb}
\lambda_i = 
\begin{cases}
\dfrac{1-|z_j-z_i|}{h}, & \forall i |\textnormal{ } |z_i-z_j|\leq h,\\
0, & \textnormal{ otherwise}.
\end{cases}
\end{equation} 
Each run of best response $K$ results in a control policy $u^K$ which is used as a constant control policy for the opponent. 

Instead of using a single update function as in \ref{eqn8}, I model the problem as solving two separate update functions over the same state-space:
\begin{equation}\label{pbell}
J_p^{(k+1,K)}(z_i)= \underset{u_1 }{\operatorname{min }}[G(z_i,u_1,u_2^K)+\gamma J_p^{(k,K)}(z_j|z_i,u_1,u_2^K)],
\end{equation}
\begin{equation}\label{ebell}
J_e^{(k+1,K)}(z_i)= \underset{u_2 }{\operatorname{max }}[G(z_i,u_1^K,u_2)+\gamma J_e^{(k,K)}(z_j|z_i,u_1^K,u_2)].
\end{equation} 
By running the update functions until $k \rightarrow \infty$, the optimal value function is achieved for a given opponents control policy $u^K$:
\begin{equation}\label{popt}
J_p^{(*,K)}(z_i)= \underset{u_1 }{\operatorname{min }}[G(z_i,u_1,u_2^K)+\gamma J_p^{(*,K)}(z_j|z_i,u_1,u_2^K)],
\end{equation}
\begin{equation}\label{eopt}
J_e^{(*,K)}(z_i)= \underset{u_2 }{\operatorname{max }}[G(z_i,u_1^K,u_2)+\gamma J_e^{(*,K)}(z_j|z_i,u_1^K,u_2)].
\end{equation}
The optimal value functions can then be used to solve for the optimal control values $u_1^{(*,K)}$ and $u_2^{(*,K)}$:
\begin{equation}\label{pcont}
u_1^{(*,K)}(z_i)= \underset{u_1 }{\operatorname{arg min }}[G(z_i,u_1,u_2^K)+\gamma J_p^{(*,K)}(z_j|z_i,u_1,u_2^K)],
\end{equation}
\begin{equation}\label{econt}
u_2^{(*,K)}(z_i)= \underset{u_2 }{\operatorname{arg max }}[G(z_i,u_1^K,u_2)+\gamma J_e^{(*,K)}(z_j|z_i,u_1^K,u_2)].
\end{equation}  

\section{Best Response}
In "Dynamic Noncooperative Game Theory", Tamer Basar and Geert Olsder prove that under certain conditions pursuit-evasion games will converge at a saddle point or Nash equilibrium. A major condition is that the value function $V$ satisfies the partial differential equation:
\begin{eqnarray}\label{pde}
-\dfrac{\delta V}{\delta t} & = & \underset{u_1 \in S_1 }{\operatorname{min }}\underset{u_1 \in S_2 }{\operatorname{max }}[\dfrac{\delta V}{\delta x}f(t,x,u_1,u_2)+g(t,x,u_1,u_2)] \\
                            & = & \underset{u_1 \in S_2 }{\operatorname{max }}\underset{u_1 \in S_1 }{\operatorname{min }}[\dfrac{\delta V}{\delta x}f(t,x,u_1,u_2)+g(t,x,u_1,u_2)].
\end{eqnarray} 
Furthermore, it can be shown that if the value function is continuously differentiable then the saddle point equilibrium exist:
\begin{theorem}[Existence of Saddle Point \cite{basar}]\label{pethrm}
If $(i)$ a continuously differentiable function $V(t,x)$ exists that satisfies the Isaacs equation \ref{pde}, $(ii)$ $V(T,x) = q(T,x)$ on the boundary of the target set, defined by $\mathscr{l}(t,x) = 0$, and $(iii)$ either $u_1^*(t) = \gamma_1^*(t,x)$, or $u_2^*(t) = \gamma_2^*(t,x)$ as derived from \ref{pde}, generates trajectories that terminate in finite time (whatever $\gamma_2$ , respectively $\gamma_1$, is) then $V(t,x)$ is the value function and the pair $(\gamma_1^*, \gamma_2^*)$ constitutes a saddle point.
\end{theorem}

Using the intuition from \Cref{pethrm}, if each iteration of best response is run until $K \rightarrow \infty$ and $u_1^{(*,K)}$ and $u_2^{(*,K)}$ each converge to a single set of control values, then those control values are considered a Nash equilibrium and:
\begin{equation}\label{pbropt}
J_p^{(*,*)}(z_i)= \underset{u_1 }{\operatorname{min }}[G(z_i,u_1,u_2^K)+\gamma J_p^{(*,*)}(z_j|z_i,u_1,u_2^*)],
\end{equation}
\begin{equation}\label{ebropt}
J_e^{(*,*)}(z_i)= \underset{u_2 }{\operatorname{max }}[G(z_i,u_1^K,u_2)+\gamma J_e^{(*,*)}(z_j|z_i,u_1^*,u_2)].
\end{equation}
The optimal control values can be determined by:
\begin{equation}\label{pbrcont}
u_1^{(*,*)}(z_i)= \underset{u_1 }{\operatorname{arg min }}[G(z_i,u_1,u_2^K)+\gamma J_p^{(*,*)}(z_j|z_i,u_1,u_2^*)],
\end{equation}
\begin{equation}\label{ebrcont}
u_2^{(*,*)}(z_i)= \underset{u_2 }{\operatorname{arg max }}[G(z_i,u_1^K,u_2)+\gamma J_e^{(*,*)}(z_j|z_i,u_1^*,u_2)].
\end{equation}
These equations result in a dynamic programming best response algorithm \ref{DPBRalg}.    
\begin{algorithm}
\caption{Best Response Dynamic Programming }\label{DPBRalg}
\begin{algorithmic}[1]
	\Require Algorithm termination criterion $\Delta_{max}$;Value Iteration termination criterion $\delta_{max}$; Initial cost functions $J_p^{(0,0)},J_e^{(0,0)}$
	\Ensure Residual $\delta=\|J_p^{(k,K)}-J_p^{(k-1),K}\|^2<\delta_{max}$;Residual $\delta=\|J_e^{(k,K)}-J_e^{(k-1),K}\|^2<\delta_{max}$;Residual $\Delta_p=\|J_p^{(*,K)}-J_p^{*,(K-1)}\|^2<\Delta_{max}$;Residual $\Delta_e=\|J_e^{(*,K)}-J_e^{*,(K-1)}\|^2<\Delta_{max}$
	\State $\Delta_p = \Delta_{max} + 1$
	\State $K = 1$
	\While{ $\Delta_p > \Delta_{max} \cup \Delta_e > \Delta_{max} $} \do{}
		\State $\delta = \delta_{max} + 1$
		\State $k = 0$
		\While{ $\delta > \delta_{max}$} \do{}
			\For{$\forall z_i$}
				\State \ref{pbell}
			\EndFor
			\State $k = k+1$
			\State $\delta = \|J_p^{(k,K)}-J_p^{(k-1),K}\|^2$
		\EndWhile
		\State $\delta = \delta_{max} + 1$
		\State $k = 0$
		\While{ $\delta > \delta_{max}$} \do{}
			\For{$\forall z_i$}
				\State \ref{ebell}
			\EndFor
			\State $k = k+1$
			\State $\delta = \|J_e^{(k,K)}-J_e^{(k-1),K}\|^2$
		\EndWhile
		\State $\Delta_p = \|J_p^{(*,K)}-J_p^{*,(K-1)}\|^2$
		\State $\Delta_e = \|J_e^{(*,K)}-J_e^{*,(K-1)}\|^2$
		\State $K = K+1$
	\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Modifications to Tensor-based Value Iteration}

Tensor-based value iteration can be modified to solve pursuit-evasion games using a best response dynamic. As in \cite{gorod}, tensor-train cross can be used to provide an approximation of value iteration within an error bound $\epsilon$. By applying tensor-train cross to \ref{DPBRalg}, a best response TVI algorithm results \ref{BRTVIalg}.
\begin{algorithm}
\caption{Best Response TVI }\label{BRTVIalg}
\begin{algorithmic}[1]
	\Require Algorithm termination criterion $\Delta_{max}$;Value Iteration termination criterion $\delta_{max}$; Initial cost functions $J_p^{(0,0)},J_e^{(0,0)}$;TT-cross accuracy $\epsilon$;
	\Ensure Residual $\delta=\|J_p^{(k,K)}-J_p^{(k-1),K}\|^2<\delta_{max}$;Residual $\delta=\|J_e^{(k,K)}-J_e^{(k-1),K}\|^2<\delta_{max}$;Residual $\Delta_p=\|J_p^{(*,K)}-J_p^{*,(K-1)}\|^2<\Delta_{max}$;Residual $\Delta_e=\|J_e^{(*,K)}-J_e^{*,(K-1)}\|^2<\Delta_{max}$
	\State $\Delta_p = \Delta_{max} + 1$
	\State $K = 1$
	\While{ $\Delta_p > \Delta_{max} \cup \Delta_e > \Delta_{max} $} \do{}
		\State $\delta = \delta_{max} + 1$
		\State $k = 0$
		\While{ $\delta > \delta_{max}$} \do{}
			\State $J_p^{(k+1),K} = \textnormal{ TT-cross }((\ref{pbell}),\epsilon)$
			\State $k = k+1$
			\State $\delta = \|J_p^{(k,K)}-J_p^{(k-1),K}\|^2$
		\EndWhile
		\State $\delta = \delta_{max} + 1$
		\State $k = 0$
		\While{ $\delta > \delta_{max}$} \do{}
			\State $J_e^{(k+1),K} = \textnormal{ TT-cross }((\ref{ebell}),\epsilon)$
			\State $k = k+1$
			\State $\delta = \|J_e^{(k,K)}-J_e^{(k-1),K}\|^2$
		\EndWhile
		\State $\Delta_p = \|J_p^{(*,K)}-J_p^{*,(K-1)}\|^2$
		\State $\Delta_e = \|J_e^{(*,K)}-J_e^{*,(K-1)}\|^2$
		\State $K = K+1$
	\EndWhile
\end{algorithmic}
\end{algorithm}
This algorithm can be used to efficiently solve a variety of problems including those presented in \Cref{chp:examples}. 